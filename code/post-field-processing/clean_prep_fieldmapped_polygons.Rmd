Clean and finalize AOP & UAS field data for the Macrosystems project

This script performs the following steps:
-Fixes all species names to ensure in scientific form
-Ensures that all categories match as they should, fixes where possible, and removes those instances that don't match (reports n removed)
-Checks for species listed as shrubs AND trees, and fixes them according to manually set rules based on output information
-Cleans up NAs and formats correctly
-Filters to only include polygons of a large enough size (manually set; reports n removed)

Tyler L. McIntosh
CU Boulder CIRES Earth Lab
Last updated: 12/19/23

This script uses the following naming conventions wherever possible:
 lowerCamelCase for variables
 period.separated for functions
 underscore_separated for files


# Set parameters for run

```{r}
#clean
rm(list = ls())

today <- "1_24_2025" #a string containing the date of run

```


# Set libraries

```{r}

#Select library chunks from R_Libraries.txt and add to "list.of.packages"

######Utility libraries
utility <- c(
 	"tictoc", #benchmarking
	"beepr", # beep() will make a noise. options(error = beep) will beep on any errors
	"zip", #For zipping files (does not require external tools). mode = "cherry-pick" to select specific files w/o mirroring directory structure
	"units", #for managing units
	"remotes", #to access github
	"here" #Relative path best practices
)

#####Data management libraries
dataManagement <- c(
	"tidyverse", #Includes: dplyr, forcats, ggplot2, lubridate, purrr, readr, stringr, tibble, tidyr
	"glue", #Easy string management
	"httr"
)

#####Geographic libraries
geographicStandard <- c(
	"terra",  #New raster data package, documentation pdf here: https://cran.r-project.org/web/packages/terra/terra.pdf
#When writing rasters, always specify data type to optimize storage & future computation:
#https://search.r-project.org/CRAN/refmans/raster/html/dataType.html
	"sf", #New vector package
	"geos", #topology operations on geometries ('rgeos' package retired in Oct 2023, transition to sf, terra, or geos packages)
  "tigris"
)

#ADD LIBRARY CHUNKS HERE
list.of.packages <- c(utility, dataManagement, geographicStandard)

```



# Setup libraries & workspace, source functions

```{r}

#Install all CRAN packages that aren't installed yet
install.packages(setdiff(list.of.packages, rownames(installed.packages())))

#Install all packages from GitHub (if any) using remotes package that aren't installed yet
if(exists("github")) { #If there are github-dependant packages...
  library(remotes) #load remotes package
  for(package in github) {
    if(! package %in% rownames(installed.packages())) {
      remotes::install_github(package) #Install package
    }
  }
  #Replace GitHub package names in list.of.packages with the actual name of the package to load
  github_names <- sub(".*/", "", github)  # Extract names after "/"
  list.of.packages <- gsub(paste(github, collapse = "|"), github_names, list.of.packages)
}

#Load all packages
invisible(lapply(list.of.packages, library, character.only = TRUE)) #use 'invisible' to prevent output being displayed


# Clean workspace & set up environment
here::here() #Check here location
here::i_am("code/post-field-processing/clean_prep_fieldmapped_polygons.Rmd")
#OR
#setwd() #Set working directory directly
options(digits = 6) #Set standard decimal print output
options(scipen = 999) #Turn scientific notation on and off (0 = on, 999 = off)

# source functions
source(here::here('code', 'functions.R'))

```

# Create necessary directories - delete any unnecessary

```{r}
# Figures directory
figsDir <- here::here("figs")
if (!dir.exists(figsDir)){
  dir.create(figsDir)
}

#Derived data directory
datDerDir <- here::here("data/derived")
if (!dir.exists(datDerDir)){
  dir.create(datDerDir)
}

```


# Access data remotely to ensure that the most recent data is being used and add necessary metadata

``` {r}

epsg <- "EPSG:5070"

states <- tigris::states() |>
  sf::st_transform(epsg)
counties <- tigris::counties() |>
  sf::st_transform(epsg) 

#Load raw data from ArcGIS Online
# aopPoly <- sf::st_read(here::here('data', 'raw', 'aop_macrosystems_data_1_7_25.geojson')) |>
#   sf::st_transform(epsg)
# uasPoly <- sf::st_read(here::here('data', 'raw', 'macrosystems_uas_polygons_2023_12_13_23.gpkg')) |>
#   sf::st_transform(epsg)

# Not necessary - made files public
##Get token - use Tyler's ArcGIS Online login info
#arc_token <- get_arcgis_online_token()


# Set parameters for both data requests
query_params <- list(where = "1=1",
                     outFields = "*",
                     f = "json")

#Get AOP polygon data from Tyler's ArcGIS online account
aop_polys_url <- "https://services.arcgis.com/b3fMqPOmotX6SV4k/ArcGIS/rest/services/niwot_aop_s_lower_hp_macrosystems_data/FeatureServer/0/QUERY"

aopPoly <- access_data_get_x_from_arcgis_rest_api_geojson(base_url = aop_polys_url,
                                                                   query_params = query_params,
                                                                   max_record = 1000,
                                                                   n = "all",
                                                                   timeout = 500) |>
  sf::st_transform(epsg)

# Get UAS polygon data from Tyler's ArcGIS online account
uas_polys_url <- "https://services.arcgis.com/b3fMqPOmotX6SV4k/ArcGIS/rest/services/macrosystems_uas_polygons_2023/FeatureServer/0/QUERY"
uasPoly <- access_data_get_x_from_arcgis_rest_api_geojson(base_url = uas_polys_url,
                                                                   query_params = query_params,
                                                                   max_record = 1000,
                                                                   n = "all",
                                                                   timeout = 500) |>
  sf::st_transform(epsg)


# Get corrected polygon data provided by Solomon Wilcox (undergrad)
# These polygons needed to be corrected due to issues with the drone flights that resulted in mapping onto RGB imagery rather than directly onto MicaSense imagery
# Solomon georeferenced the RGB imagery to the MicaSense imagery and then moved the polygons to correctly correspond
# For the Kremmling area and the Beartooth Area, these geometries need to be used instead of those in the main files
# Solomon exported data as .shp files when he worked with them, meaning that long attribute names got shortened. The columns are manually re-named here


#Kremmling
krem_uas_polys_url <- "https://services.arcgis.com/b3fMqPOmotX6SV4k/arcgis/rest/services/kremmling_polygons_FINAL/FeatureServer/0/QUERY"
krem_uas_poly <- access_data_get_x_from_arcgis_rest_api_geojson(base_url = krem_uas_polys_url,
                                                                   query_params = query_params,
                                                                   max_record = 1000,
                                                                   n = "all",
                                                                   timeout = 500) |>
  sf::st_transform(epsg) |>
  dplyr::rename(CreationDate = CreationDa,
                cover_subcategory = cover_subc,
                cover_category = cover_cate,
                og_flight_date = og_flight_,
                collector_name = collector_,
                collection_date = collection,
                description_notes = descriptio,
                woody_shrub_height = woody_shru,
                other_species = other_spec,
                other_subcategory = other_subc,
                dead_subcategory = dead_subca,
                evt_class_field = evt_class_,
                tree_height = tree_heigh) |>
  sf::st_join(states |>
                dplyr::select(STUSPS), join = st_within) |>
  dplyr::rename(state = STUSPS) |>
  dplyr::mutate(aop_site = dplyr::case_when(
                  state == "WY" ~ "YELL",
                  state == "WA" ~ "WREF",
                  state == "CO" ~ "NIWO/RMNP",
                  TRUE ~ NA_character_
                ),
                imagery = "UAS",
                field_year = lubridate::year(CreationDate))

#Beartooth - Solomon left these polygons with all the others, so the edited polygons need to be extracted
#some of the polygons also accidentally got duplicated, so the duplicates need to be removed
bear_uas_polys_url <- "https://services.arcgis.com/b3fMqPOmotX6SV4k/arcgis/rest/services/beartooth_polygons_FINAL/FeatureServer/0/QUERY"
bear_uas_poly <- access_data_get_x_from_arcgis_rest_api_geojson(base_url = bear_uas_polys_url,
                                                                   query_params = query_params,
                                                                   max_record = 1000,
                                                                   n = "all",
                                                                   timeout = 500) |>
  sf::st_transform(epsg) |>
  dplyr::distinct() |>
  dplyr::select(-Shape__Area, -Shape__Length) |>
  dplyr::rename(Shape__Area = Shape__Are,
                Shape__Length = Shape__Len,
                CreationDate = CreationDa,
                cover_subcategory = cover_subc,
                cover_category = cover_cate,
                og_flight_date = og_flight_,
                collector_name = collector_,
                collection_date = collection,
                description_notes = descriptio,
                woody_shrub_height = woody_shru,
                other_species = other_spec,
                other_subcategory = other_subc,
                dead_subcategory = dead_subca,
                evt_class_field = evt_class_,
                tree_height = tree_heigh)

bear_uas_poly_clean <- bear_uas_poly |>
  sf::st_join(states |>
                dplyr::select(STUSPS), join = st_within) |>
  dplyr::rename(state = STUSPS) |>
  dplyr::filter(state == "WY") |>
  dplyr::mutate(aop_site = dplyr::case_when(
                  state == "WY" ~ "YELL",
                  state == "WA" ~ "WREF",
                  state == "CO" ~ "NIWO/RMNP",
                  TRUE ~ NA_character_
                ),
                imagery = "UAS",
                field_year = lubridate::year(CreationDate))



#Check polygon validity
unique(sf::st_is_valid(aopPoly))
unique(sf::st_is_valid(uasPoly))

#Add in attribute to specify what dataset it is (for later joining) & cast to ensure polygons, add field season year
aopPoly <- aopPoly |>
  dplyr::mutate(imagery = "AOP",
                field_year = lubridate::year(CreationDate)) |>
  sf::st_cast("POLYGON")
uasPoly <- uasPoly |>
  dplyr::mutate(imagery = "UAS",
                field_year = lubridate::year(CreationDate)) |>
  sf::st_cast("POLYGON")

# Add state/site
aopPoly <- aopPoly |>
  sf::st_join(states |> dplyr::select(STUSPS), join = st_within) |>
  dplyr::rename(state = STUSPS) |>
  dplyr::mutate(aop_site = dplyr::case_when(
    state == "WY" ~ "YELL",
    state == "WA" ~ "WREF",
    state == "CO" ~ "NIWO/RMNP",
    TRUE ~ NA_character_
  ))

uasPoly <- uasPoly |>
  sf::st_join(states |> dplyr::select(STUSPS), join = st_within) |>
  dplyr::rename(state = STUSPS) |>
  dplyr::mutate(aop_site = dplyr::case_when(
    state == "WY" | state == "MT" ~ "YELL",
    state == "WA" | state == "OR" ~ "WREF",
    state == "CO" ~ "NIWO/RMNP",
    TRUE ~ NA_character_
  ))


#ADD THE MANUALLY-CORRECTED POLYGONS FROM SOLOMON BACK TO THE MAIN GROUP
uasPoly <- uasPoly |>
  dplyr::filter(state != "WY") |>
  dplyr::bind_rows(bear_uas_poly_clean) |>
  dplyr::bind_rows(krem_uas_poly) |>
  dplyr::select(-FID)



```

# Check for inconsistencies in the data

```{r}
#Check data
print("AOP")
unique(aopPoly$species) #still includes species common names, needs fixed; also there are some that have species in the 'other species' column that we do have in our list now, need to add to species data
unique(aopPoly$cover_category)
unique(aopPoly$cover_subcategory)
unique(aopPoly$dead_subcategory)

print("UAS")
unique(uasPoly$species) #all scientific names, good to go
unique(uasPoly$cover_category)
unique(uasPoly$cover_subcategory)
unique(uasPoly$dead_subcategory)

```

# Fix AOP species data from before we started using scientific names

```{r}

#create species field replacement data (from before scientific species dropdown implementation)
replacements <- cbind(
  c("Aspen", "Ponderosa Pine", "Lodgepole Pine", "Fir", "Douglas-Fir", "Pine other", "Spruce", "Juniper", "Limber Pine", "Englemann Spruce", "Subalpine Fir", "Spruce - Unidentified", "Blue Spruce", "Maple", "Pine - Unidentified", "Deciduous - Unidentified", "Rocky Mountain Maple", "Chokecherry", "Other", "Willow", "Thinleaf Alder"),
  c("Populus tremuloides", "Pinus ponderosa", "Pinus contorta", "Abies", "Pseudotsuga menziesii", "Pinus", "Picea", "Juniperus scopulorum", "Pinus flexilis", "Picea engelmannii", "Abies lasiocarpa", "Picea", "Picea pungens", "Acer glabrum", "Pinus", "Unidentified", "Acer glabrum", "Prunus virginiana", "Other", "Salix", "Alnus tenuifolia")
) |>
  as.data.frame() |>
  `names<-`(c("species", "newSpecies"))

#create 'other_species' field replacement data for AOP (from before scientific species dropdown implementation)
oReplacements <- cbind(
  c("Skunkbush Sumac", "Wax Currant ", "Boulder Raspberry ", "Cliff Jamecia ", "Currant", "Common Juniper", "Chokecherry", "Limber pine"),
  c("Rhus aromatica", "Ribes cereum", "Rubus deliciosus", "Jamesia americana", "Ribes", "Juniperus communis", "Prunus virginiana", "Pinus flexilis")
) |>
  as.data.frame() |>
  `names<-`(c("other_species", "newSpecies"))

#Make replacements in aopPolygons
aopPoly <- aopPoly |>
  dplyr::left_join(replacements, by = c("species")) |>
  dplyr::mutate(species = dplyr::case_when(is.na(newSpecies) ~ species,
                                    TRUE ~ newSpecies)) |>
  dplyr::select(-newSpecies) |>
  dplyr::left_join(oReplacements, by = c("other_species")) |>
  dplyr::mutate(species = dplyr::case_when(is.na(newSpecies) ~ species,
                                           TRUE ~ newSpecies)) |>
  dplyr::select(-newSpecies)


unique(aopPoly$species) #fixed!
filter(aopPoly, !is.na(other_species)) #fixed!

```
# Check for category matches, fix and remove entries

```{r}
print("AOP Polygon category sets")
aopPoly |> dplyr::distinct(cover_category, cover_subcategory, dead_subcategory)

print("UAS Polygon category sets")
uasPoly |> dplyr::distinct(cover_category, cover_subcategory, dead_subcategory)


#There are a few mistakes in both datasets. These are regarding "Dead" and woody shrub subcategories, as well as species for things that shouldn't have them. Function to apply fix and remove rules
#Fix rules -
# anything with "Dead" in cover_subcategory should have cover_category == "Non-vegetated & dead"
# any woody shrubs, or deciduous/evergreen trees should be "Unidentified" species, not NA
#Remove rules -
# evergreen : Woody shrub - Broadleaf
# evergreen : Woody shrub - Needleleaf
# deciduous : woody shrub - Broadleaf
# herbaceous : woody shrub - Needleleaf
# herbaceous : woody shrub - Broadleaf
# Non-vegetated & dead : Bare : Dead - Woody downed
# Herbaceous : contains species
# Non-vegetated & dead : contains species
clean.remove <- function(dats) {
  out <- dats |>
    dplyr::mutate(cover_category = dplyr::case_when(cover_subcategory == "Dead" ~ "Non-vegetated & dead",
                                                    TRUE ~ cover_category),
                  species = dplyr::case_when(cover_category == "Woody shrub" & is.na(species) ~ "Unidentified",
                                                    TRUE ~ species)) |>
    dplyr::filter(! ((cover_category == "Evergreen" & cover_subcategory == "Woody shrub - Broadleaf") |
                     (cover_category == "Evergreen" & cover_subcategory == "Woody shrub - Needleleaf") |
                     (cover_category == "Deciduous" & cover_subcategory == "Woody shrub - Broadleaf") |
                     (cover_category == "Herbaceous" & cover_subcategory == "Woody shrub - Needleleaf") |
                     (cover_category == "Herbaceous" & cover_subcategory == "Woody shrub - Broadleaf") |
                     (cover_subcategory == "Bare" & dead_subcategory == "Dead - Woody downed") |
                     (cover_category == "Herbaceous" & !is.na(species)) |
                     (cover_category == "Non-vegetated & dead" & !is.na(species))))
  return(out)
}

#Apply function
aopPolyClean <- aopPoly |> clean.remove()
uasPolyClean <- uasPoly |> clean.remove()

glue::glue("{nrow(aopPoly) - nrow(aopPolyClean)} polygons were removed from the AOP data")
glue::glue("{nrow(uasPoly) - nrow(uasPolyClean)} polygons were removed from the UAS data")

```

# Check for species that are recorded as both shrubs and trees and deal with accordingly

```{r}
aopPolyClean |> dplyr::distinct(cover_category, species) |> dplyr::arrange(species)
  #Acer glabrum - Rocky Mountain Maple

uasPolyClean |> dplyr::distinct(cover_category, species) |> dplyr::arrange(species)
  #Juniperus scopulorum - Rocky Mountain Juniper
  #Quercus gambelii - Gambel oak

#A function to analyze what is going on with the species that are listed as both trees and shrubs
analyze.shrub.tree.species <- function(speciesNm) {
  combinedSpecies <- rbind(aopPolyClean |> dplyr::filter(species == speciesNm),
                           uasPolyClean |> dplyr::filter(species == speciesNm))
  print(speciesNm)
  print(glue::glue("There are {nrow(combinedSpecies)} instances of {speciesNm} across both datasets."))
  print(glue::glue("Of those, {nrow(filter(combinedSpecies, imagery == 'AOP'))} are in the AOP data and {nrow(filter(combinedSpecies, imagery == 'UAS'))} are in the UAS data"))
  print(glue::glue("{nrow(filter(combinedSpecies, cover_category == 'Woody shrub'))} of the instances are classified as woody shrubs, while {nrow(filter(combinedSpecies, cover_category == 'Evergreen'))} are classified as evergreen trees and {nrow(filter(combinedSpecies, cover_category == 'Deciduous'))} are classified as deciduous trees"))
  print(glue::glue("{nrow(filter(combinedSpecies, cover_category == 'Woody shrub' & woody_shrub_height == '10cm to 1m'))} of the shrubs are 10cm to 1m, while {nrow(filter(combinedSpecies, cover_category == 'Woody shrub' & woody_shrub_height == 'Over 1m'))} of the shrubs are over 1m"))
  summary <- c(speciesNm,
               nrow(combinedSpecies),
               nrow(filter(combinedSpecies, imagery == 'AOP')),
               nrow(filter(combinedSpecies, imagery == 'UAS')),
               nrow(filter(combinedSpecies, cover_category == 'Woody shrub')),
               nrow(filter(combinedSpecies, cover_category == 'Evergreen')),
               nrow(filter(combinedSpecies, cover_category == 'Deciduous')),
               nrow(filter(combinedSpecies, cover_category == 'Woody shrub' & woody_shrub_height == '10cm to 1m')),
               nrow(filter(combinedSpecies, cover_category == 'Woody shrub' & woody_shrub_height == 'Over 1m'))) |>
    t() |>
    as.data.frame() |>
    `names<-`(c("Species",
                "Instances",
                "AOPinstances",
                "UASinstances",
                "WoodyShrubInstances",
                "EvergreenInstances",
                "DeciduousInstances",
                "WoodyShrubInstances10cm1m",
                "WoodyShrubInstancesOver1m"))
  return(summary)
}

#Get all information on our shrub-trees
shrubTreeSummary <- rbind(analyze.shrub.tree.species("Acer glabrum"),
                          analyze.shrub.tree.species("Juniperus scopulorum"),
                          analyze.shrub.tree.species("Quercus gambelii"))

#Decision rule function for shrub-tree species
 #Acer glabrum (Rocky mountain maple) to woody shrub
 #Juniperus scopulorum (Rocky mountain juniper) to evergreen
 #Quercus gambelii (Gambel oak) to woody shrub
fix.shrub.trees <- function(dat) {
  out <- dat |>
    dplyr::mutate(cover_category = dplyr::case_when(species == "Acer glabrum"~ "Woody shrub",
                                                    species == "Juniperus scopulorum" ~ "Evergreen",
                                                    species == "Quercus gambelii" ~ "Woody shrub",
                                                    TRUE ~ cover_category))
  return(out)
}

#Apply to data
aopPolyClean <- aopPolyClean |> fix.shrub.trees()
uasPolyClean <- uasPolyClean |> fix.shrub.trees()

```

# Clean up NA's, create merged land cover attribute for ease of use, add area

```{r}

#Clean up NA's
replace.na <- function(dat) {
  return(dat |>
           dplyr::mutate(dead_subcategory = ifelse(dead_subcategory == "NA", NA, dead_subcategory),
                         cover_subcategory = ifelse(cover_subcategory == "NA", NA, cover_subcategory)))
}

#Create a single attribute that combines all of the category and subcategory data
add.merged.land.cover <- function(dats) {
  return(dats |> dplyr::mutate(combined_all_category_species = glue::glue("{cover_category}_{cover_subcategory}_{dead_subcategory}_{species}", .na = "", .null = NULL)))
}


#Use functions
aopPolyClean <- aopPolyClean |>
  replace.na() |>
  add.merged.land.cover() |>
  st_area_to_poly(nm = "area_m")
uasPolyClean <- uasPolyClean |>
  replace.na() |>
  add.merged.land.cover() |>
  st_area_to_poly(nm = "area_m")



```

# DEPRECTATED -- Filter out polygons that are too small
This has been removed to retain all polygons for as long as possible

```{r}

# #Filter to only include polygons over 1 hyperspec pixel (1m^2)
# aopLargeEnoughClean <- aopPolyClean |>
#   dplyr::filter(area_m >= 1)
# 
# #Filter to only include polygons over 4 micasense pixels (assuming 8cm pixel, or 256cm^2)
# uasLargeEnoughClean <- uasPolyClean |>
#   dplyr::filter(area_m >= 0.0256)
# 
# glue::glue("{nrow(aopPolyClean) - nrow(aopLargeEnoughClean)} polygons were removed from the AOP data")
# glue::glue("{nrow(uasPolyClean) - nrow(uasLargeEnoughClean)} polygons were removed from the UAS data")
# 
# 
# aopLargeEnoughClean |> filter(species == "Other")

```

# Create 1/2 minimum diameter polygons sets

Note: for some unknown reason the geos::geos_maximum_inscribed_crc function does NOT work on some polygons if the entire set is used. Original theory was that it was a 2023/2024 split, but that doesn't resolve all of it. Nor does dividing by region. As a result, diameter calculations are now done at the per-polygon level

```{r half-diameter}

#Use functions iteratively on each polygon
aopPolyCleanHalf <- aopPolyClean %>%
  split(seq_len(nrow(.))) |>
  purrr::map(buffer_to_half_diam, 0.01) |>
  dplyr::bind_rows()

uasPolyCleanHalf <- uasPolyClean %>%
  split(seq_len(nrow(.))) |>
  purrr::map(buffer_to_half_diam, 0.01) |>
  dplyr::bind_rows()

# #Use function
# uasPolyCleanHalf <- buffer.set.to.half.diam(uasPolyClean, 0.01)
# aopPolyCleanHalf <- buffer.set.to.half.diam(aopPolyClean, 0.01)

mapview(uasPolyClean) + mapview(uasPolyCleanHalf, col.regions = "red")

```


# Write out datasets for use

```{r}
#Write out
sf::st_write(uasPolyClean, here::here(datDerDir, paste0("uas_polygons_", today, "_analysis_ready.geojson")), append = FALSE)
sf::st_write(uasPolyCleanHalf, here::here(datDerDir, paste0("uas_polygons_", today, "_analysis_ready_half_diam.geojson")), append = FALSE)

#Write out
sf::st_write(aopPolyClean, here::here(datDerDir, paste0("aop_polygons_", today, "_analysis_ready.geojson")), append = FALSE)
sf::st_write(aopPolyCleanHalf, here::here(datDerDir, paste0("aop_polygons_", today, "_analysis_ready_half_diam.geojson")), append = FALSE)

```